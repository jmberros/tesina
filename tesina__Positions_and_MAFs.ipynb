{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General settings and loading of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:52] 'panels' dict\n",
      "[20:34:52] 'galanter', 'present', 'missing' dataframes\n",
      "[20:34:52] 'panel_labels'\n",
      "[20:34:52] 'panel_names' dict\n",
      "[20:34:52] 'genome' dataframe\n",
      "[20:35:00] 'lat' dataframe"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "%run helpers/data_munging_functions.py\n",
    "%run helpers/number_helpers.py\n",
    "%run general_settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "for label, panel in panels.items():\n",
    "    count = len(panel)\n",
    "    percentage = round(100 * count / len(galanter), 1)\n",
    "    percentage = \"{}%\".format(percentage)\n",
    "\n",
    "    res[label] = {\n",
    "        \"AIMs count\": count,\n",
    "        \"AIMs percentage\": percentage,\n",
    "    }\n",
    "\n",
    "aim_count = pd.DataFrame(res).transpose()\n",
    "aim_count.sort_index(ascending=False, inplace=True)\n",
    "aim_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chequear de qué poblaciones ancestrales son los AIMs en GAL_Affy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for label, panel in panels.items():\n",
    "    if \"Ausentes\" in label:\n",
    "        continue\n",
    "    series = panel['population'].value_counts()\n",
    "    series.name = label\n",
    "    series_percentage = (100 * series / len(panel)).apply(lambda x: \"{}%\".format(round(x, 1)))\n",
    "    res.extend([series, series_percentage])\n",
    "\n",
    "df = pd.concat(res, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En GAL_Affy observamos un aumento de la proporción de AIMs africanos respecto de los europeos, mientras que la proporción de AIMs americanos sigue igual. Esto podría tener por consecuencia, a primera vista, una sobreestimación del componente africano al usar GAL_Affy. No obstante, debe calcularse el LSBL acumulado de los SNPs de cada grupo y comparar *ese* valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparar el LSBL de los paneles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "for label, panel in panels.items():\n",
    "    print(label)\n",
    "    html = panel.groupby(\"population\").sum()[[\"LSBL(Fst)\", \"LSBL(In)\"]].to_html()\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los AIMs que quedan en GAL_Affy siguen balanceados en cuanto a LSBL para determinar los componentes EUR y NAM. EL valor de LSBL queda relativamente más alto que en GAL_Completo, por lo que debemos estar atentos a posibles diferencias en resultado entre GAL_Affy y GAL_Completo que afecten específicamente al componente ancestral africano --en particular, que lo sobreestimen.\n",
    "\n",
    "Sin embargo, esto no parece ser un problema en nuestros resultados, al menos para las muestras de 1000 Genomas utilizadas. La comparación de proporción de ancestría africana estimada para las diferentes poblaciones parece mantenerse igual entre ambos paneles, a pesar de esa reducción diferencial de AIMs. Con todo, este _caveat_ quedará para futuros usos de GAL_Affy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparar Galanter de PLoS / dbSNP / ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter_remapped = pd.read_csv(\"/home/juan/tesina/files/galanter.GRCh38.p6.bed\", sep=\"\\t\",\n",
    "                               names=['chr', 'position', 'pos_to', 'id'], comment=\"#\", skiprows=3,\n",
    "                               usecols=['chr', 'position', 'pos_to', 'id'])\n",
    "\n",
    "# The online remapping generated weird chromosomes\n",
    "weird_chromosomes = galanter_remapped[\"chr\"].str.contains(\"_\")\n",
    "galanter_remapped.drop(galanter_remapped[weird_chromosomes].index, inplace=True)\n",
    "\n",
    "galanter_remapped['chr'] = galanter_remapped['chr'].str.replace(\"chr\", \"\").astype(int)\n",
    "galanter_remapped.drop_duplicates(\"id\", keep=\"last\", inplace=True)\n",
    "galanter_remapped.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter_plos = galanter[['chr', 'position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "galanter_dbSNP = pd.read_csv(\"/home/juan/tesina/files/galanter_rsIDs_dbSNP\",\n",
    "                             names=['id', 'source', 'chr', 'pos', 'pos_to'], skiprows=1).set_index('id')\n",
    "take_these_out = galanter_dbSNP.chr.str.contains('HSCHR')\n",
    "galanter_dbSNP = galanter_dbSNP[~take_these_out]\n",
    "galanter_dbSNP['chr'] = galanter_dbSNP['chr'].astype(int)\n",
    "galanter_dbSNP = galanter_dbSNP.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter.index.difference(galanter_remapped.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debería usar las posiciones de el build `GRCh38.p6` en `galanter_remapped`, pero no están todas puede ser? Chequear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(galanter.index.difference(galanter_remapped.index))\n",
    "print(galanter_remapped.index.difference(galanter.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficar la distribución cromosómica de los AIMs\n",
    "\n",
    "* Chromosome lengths were taken from:\n",
    "\n",
    "[Assembly Statistics for GRCh38.p6 Release date: December 23, 2015](http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/data/)\n",
    "\n",
    "* Centromere ranges were taken from the FTP server: \n",
    "\n",
    "ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_000001405.21_GRCh38.p6/GCA_000001405.21_GRCh38.p6_assembly_structure/genomic_regions_definitions.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancias entre los AIMs por cromosoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% run data_munging/distances.py\n",
    "% run SNP_distances_plots.py\n",
    "\n",
    "ax1 = plt.subplot(211)\n",
    "ax1 = distances_boxplot(galanter, genome, ax=ax1,\n",
    "                        title=r\"Distancia entre AIMs en GAL_Completo\")\n",
    "\n",
    "ax2 = plt.subplot(212, sharey=ax1)\n",
    "ax2 = distances_boxplot(present, genome, ax=ax2,\n",
    "                        title=r\"Distancia entre AIMs en GAL_Affy\")\n",
    "\n",
    "hide_spines_and_ticks(ax1)\n",
    "hide_spines_and_ticks(ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances = {}\n",
    "\n",
    "for panel_name, panel in {\"GAL_Completo\": galanter, \"GAL_Affy\": present}.items():\n",
    "    temp_dict = snp_distances_per_chromosome(panel, genome)\n",
    "\n",
    "    for chromosome, chr_distances in temp_dict.items():\n",
    "        temp_dict[chromosome] = (np.mean(chr_distances),\n",
    "                                 np.std(chr_distances),\n",
    "                                 np.median(chr_distances))\n",
    "    \n",
    "    distances[panel_name] = pd.DataFrame(temp_dict).T.astype(int)\n",
    "    distances[panel_name].columns = [\"mean\", \"std\", \"median\"]\n",
    "\n",
    "distances_df = distances[\"GAL_Completo\"].join(distances[\"GAL_Affy\"], rsuffix=\"_Affy\")\n",
    "\n",
    "# Pensé que representarlo en una tabla sería mejor, pero tal vez no.\n",
    "# El boxplot es más informativo más rápidamente, y más intuitivo.\n",
    "# distances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo `galanter.bed` con las posiciones de los AIMs reportadas por Galanter *et al.* correspondía al genoma de referencia `GRCh37.p5`. La conversión al build `GRCh38.p6` fue realizada online con la herramienta de *remapping* provista por NCBI Genome Tools, de modo que fueran coherentes con la información sobre longitud total de los cromosomas y rango de los centrómeros.\n",
    "\n",
    "http://www.ncbi.nlm.nih.gov/genome/tools/remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "galanter[\"pos\"] = galanter_remapped[\"position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pop_counts(df):\n",
    "    return df.groupby(['chr', 'population']).size().unstack().fillna('>> 0 <<')\n",
    "\n",
    "df = pd.merge(pop_counts(galanter), pop_counts(present),\n",
    "         left_index=True, right_index=True,\n",
    "         suffixes=['_TOTAL', '_IN'])\n",
    "\n",
    "df.columns = pd.MultiIndex.from_product([['$GALANTER_{TOTAL}$', '$GALANTER_{IN}$'],\n",
    "                                         ['AFR', 'EUR', 'NAM']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCUSIÓN**\n",
    "\n",
    "La reducción de AIMs en GALANTER-IN determina que ciertos cromosomas no tengan ningún AIM de una población determinada:\n",
    "\n",
    "- En el cromosoma 12 se perdieron los 3 AIMs de NAM.\n",
    "- En el cromosoma 20 se perdió el único AIM de AFR.\n",
    "- En el cromosoma 22 se perdieron los 2 AIMs de EUR.\n",
    "\n",
    "En realidad esto no importa demasiado, ya que estamos haciendo inferencia de ancestría global, no local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate CONTROL PANELS of random SNPs\n",
    "\n",
    "- Extract the **same amount * factor** of SNPs per chromosome as Galanter has,\n",
    "  with some min space between them.\n",
    "- The SNPs to extract should all be present in LAT-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter.groupby(\"chr\").size().to_csv(\"data/chr_SNP_count_in_galanter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# === WARNING ====\n",
    "# Running this cell took 26 minutes. I already got the spn_lists written to files\n",
    "# in the following cell, so there's no reason to run this again.\n",
    "\n",
    "\n",
    "# from math import floor\n",
    "# import time\n",
    "\n",
    "# control_panels = {}\n",
    "\n",
    "# for exponent in [0, 1, 2]:\n",
    "#     factor = 10 ** exponent\n",
    "#     print(\"== Control Panel x {} ==\\n\".format(factor))\n",
    "#     number_of_snps_to_take = galanter.groupby(\"chr\").size() * factor\n",
    "\n",
    "#     control_panels[factor] = {}\n",
    "    \n",
    "#     for chromosome, snps_number in number_of_snps_to_take.items():            \n",
    "#         this_chromosome = lat[lat[\"Chromosome\"] == str(chromosome)]\n",
    "#         positions = this_chromosome[\"Position End\"]\n",
    "        \n",
    "#         # I will maximize the distance between indices of the positions list\n",
    "#         # as a proxy to maximize the distance between the positions in the chromosome ;)\n",
    "#         distance_between_indices = floor(len(positions) / snps_number)\n",
    "#         positions_to_take = [positions[n * distance_between_indices]\n",
    "#                              for n in np.arange(snps_number)]\n",
    "\n",
    "#         indices_to_take = positions[positions.isin(positions_to_take)].index.unique()\n",
    "#         control_panels[factor][chromosome] = indices_to_take\n",
    "        \n",
    "#         print(\"[{}] Chr {}, {} to take, {} were taken\".format(time.strftime(\"%H:%M:%S\"),\n",
    "#                                                               chromosome, snps_number,\n",
    "#                                                               len(control_panels[factor][chromosome])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# for factor, snp_dic in control_panels.items():\n",
    "#     for chromosome, snp_list in snp_dic.items():\n",
    "#         basedir = \"/home/juan/tesina/files\"\n",
    "#         fn = \"control_panel_x{}.chr_{}.snp_list_to_take_{}\".format(factor, chromosome, len(snp_list))\n",
    "#         with open(os.path.join(basedir, fn), \"w\") as dest_file:\n",
    "#             dest_file.write(\"\\n\".join(snp_list) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ With the generated snp_lists per chromosome, we need to extract those variants\n",
    "from the 1000 Genomes `*.vcf` files, using PLINK.\n",
    "\n",
    "The script used with this purpose is `/home/juan/tesina/1000 ... /create_control_panels.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Control Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# control_names = OrderedDict()\n",
    "\n",
    "# for factor in cp_factors:\n",
    "#     snp_count = len(control_genotypes[factor].columns)\n",
    "#     name = \"Panel de {0:,} SNPs\".format(snp_count)\n",
    "#     control_names[factor] = name.replace(\",\", \".\")\n",
    "    \n",
    "control_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the distribution of AIMs per chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "control_positions = OrderedDict()\n",
    "\n",
    "for factor in cp_factors:\n",
    "    rs_ids = control_genotypes[factor].columns\n",
    "    df = lat.loc[rs_ids][[\"Chromosome\", \"Position End\"]]\n",
    "    df.columns = [\"chr\", \"position\"]\n",
    "    df[\"position\"] = df[\"position\"].astype(int)\n",
    "    df[\"chr\"] = df[\"chr\"].astype(int)\n",
    "    df.index.name = \"rsID\"\n",
    "    control_positions[factor] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% run chromosomes_with_SNPs_plot.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "markersize = 40\n",
    "\n",
    "plot_settings = [\n",
    "    # GAL_Completo\n",
    "    OrderedDict([\n",
    "        ('AIMs de GAL_Completo',\n",
    "         {'df': galanter,\n",
    "          'marker': '_', 'color': 'greenyellow', 's': markersize}),\n",
    "    ]),\n",
    "    \n",
    "    # GAL_Affy\n",
    "    OrderedDict([\n",
    "        ('AIMs de GAL_Affy',\n",
    "         {'df': present,\n",
    "          'marker': '_', 'color': 'greenyellow', 's': markersize}),\n",
    "    ]),\n",
    "    \n",
    "    # GAL_Completo vs. GAL_Affy\n",
    "    OrderedDict([\n",
    "        ('AIMs de GAL_Affy',\n",
    "         {'df': present,\n",
    "          'marker': '_', 'color': 'greenyellow', 's': markersize}),\n",
    "        ('AIMs de GAL_Completo\\nque no están en LAT 1',\n",
    "         {'df': missing,\n",
    "          'marker': '_', 'color': 'tomato', 's': markersize}),\n",
    "    ]),\n",
    "    \n",
    "    # GAL_Affy different ancestries\n",
    "    OrderedDict([\n",
    "        ('GAL_Affy - AFR',\n",
    "         {'df': present[present.population == \"AFR\"],\n",
    "          'marker': '_', 'color': 'greenyellow', 's': markersize}),\n",
    "\n",
    "        ('GAL_Affy - EUR',\n",
    "         {'df': present[present.population == \"EUR\"],\n",
    "          'marker': '_', 'color': 'cyan', 's': markersize}),\n",
    "\n",
    "        ('GAL_Affy - NAM',\n",
    "         {'df': present[present.population == \"NAM\"],\n",
    "          'marker': '_', 'color': 'pink', 's': markersize}),\n",
    "    ]),\n",
    "    \n",
    "    # Control Panel x 1\n",
    "    OrderedDict([\n",
    "        ('Control Panel x 1',\n",
    "         {'df': control_positions[\"1\"],\n",
    "          'marker': '_', 'color': 'greenyellow', 's': markersize}),\n",
    "    ]),\n",
    "\n",
    "    # Control Panel x 10\n",
    "    OrderedDict([\n",
    "        ('Control Panel x 10',\n",
    "         {'df': control_positions[\"10\"],\n",
    "          'marker': '_', 'color': 'greenyellow', 's': markersize}),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "for plot_data in plot_settings:\n",
    "    chromosomes_with_SNPs_plot(genome, plot_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Traer la data de 1000Genomes para estos SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load /home/juan/tesina/1000genomes/ftp_download_1000_genomes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# % run extract_SNPs_from_vcf_chromosomes.py\n",
    "\n",
    "# commands = extract_SNPs_from_vcf(galanter.index.values)\n",
    "\n",
    "# ^ One time only run, to extract the SNPs out of the big files of 1000genomes:\n",
    "# run_commands(commands, \"/home/juan/tesina/1000genomes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run read_samples_data.py\n",
    "\n",
    "sample_populations_file = \"/home/juan/tesina/1000Genomes_data/original-1000Genomes-files/\" + \\\n",
    "                          \"integrated_call_samples_v3.20130502.ALL.panel\"\n",
    "df_1000G_sample_populations = read_samples_data(sample_populations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run read_1000genomes_data.py\n",
    "%run helpers/data_munging_functions.py\n",
    "\n",
    "import itertools\n",
    "# from glob import glob\n",
    "from os.path import isfile\n",
    "\n",
    "\n",
    "# vcf_filenames = glob(\"/home/juan/tesina/1000G_analysis/galanter_1000Genomes.vcf\")\n",
    "vcf_filename = \"/home/juan/tesina/1000G_analysis/galanter_1000Genomes.vcf\"\n",
    "df_1000G_SNPs_dumpfile = \"dumpfiles/1000G_SNPinfo_dataframe.csv\"\n",
    "df_1000G_genotypes_dumpfile = \"dumpfiles/1000G_genotypes_dataframe.csv\"\n",
    "\n",
    "if not isfile(df_1000G_SNPs_dumpfile):\n",
    "    # records = [_vcf_records(vcf_filename) for vcf_filename in vcf_filenames]\n",
    "    records = _vcf_records(vcf_filename)\n",
    "    # records = itertools.chain.from_iterable(records)  # Flattens list of lists\n",
    "    records_as_dictionaries = [_vcf_record_to_dict(r) for r in records]\n",
    "\n",
    "    # Clean up 1000genomes data\n",
    "    df_1000G_SNPs = pd.DataFrame(records_as_dictionaries).set_index('ID')\n",
    "    df_1000G_SNPs = df_1000G_SNPs.dropna(axis=1)\n",
    "    df_1000G_SNPs = df_1000G_SNPs.drop(['FILTER', 'alleles'], axis=1)\n",
    "    df_1000G_SNPs = remove_unkown_snp_subtypes(df_1000G_SNPs)\n",
    "    df_1000G_SNPs = remove_unnecessary_lists_from_df(df_1000G_SNPs)\n",
    "\n",
    "    # Get sample genotypes\n",
    "    frames = [pd.DataFrame(dict(genotypes), index=[rs])\n",
    "              for rs, genotypes in df_1000G_SNPs['sample_genotypes'].iteritems()]\n",
    "    df_1000G_genotypes = pd.concat(frames).transpose()\n",
    "    df_1000G_genotypes.to_csv(df_1000G_genotypes_dumpfile)\n",
    "\n",
    "    # Remove big unnecessary field after exporting its data to 'samples_genotypes'\n",
    "    df_1000G_SNPs = df_1000G_SNPs.drop('sample_genotypes', axis=1)\n",
    "    df_1000G_SNPs.to_csv(df_1000G_SNPs_dumpfile)\n",
    "\n",
    "df_1000G_SNPs = pd.read_csv(df_1000G_SNPs_dumpfile, index_col='ID')\n",
    "df_1000G_genotypes = pd.read_csv(df_1000G_genotypes_dumpfile, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run population_names.py\n",
    "\n",
    "dumpfile = \"dumpfiles/population_names.csv\"\n",
    "# !rm dumpfiles/population_names.csv  # flush cache ;)\n",
    "df_1000G_population_names = create_population_names_df(dumpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def whois(pop_code):\n",
    "    return df_1000G_population_names.loc[pop_code]['Population Description']\n",
    "\n",
    "whois('CEU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "\n",
    "df = df_1000G_genotypes\n",
    "df2 = pd.DataFrame(index=df.index)\n",
    "\n",
    "alleles_dumpfile = 'dumpfiles/1000G_genotypes_alleles_dataframe'\n",
    "\n",
    "if not isfile(alleles_dumpfile):\n",
    "    def genotype_code_to_alleles(code, ref, alt):\n",
    "        if code == 0:\n",
    "            alleles = (ref, ref)\n",
    "        elif code == 1:\n",
    "            alleles = (ref, alt)\n",
    "        elif code == 2:\n",
    "            alleles = (alt, alt)\n",
    "        else:\n",
    "            raise ValueError(\"I don't know genotype '{}'\".format(code))\n",
    "\n",
    "        return ''.join(alleles)\n",
    "\n",
    "    for i, (rs, genotypes) in enumerate(df.iteritems()):\n",
    "        ref, alt = df_1000G_SNPs.loc[rs][['REF', 'ALT']]\n",
    "        df2[rs] = genotypes.apply(genotype_code_to_alleles, args=(ref, alt))\n",
    "\n",
    "    df2.to_csv(alleles_dumpfile)\n",
    "\n",
    "df_1000G_genotypes_alleles = pd.read_csv(alleles_dumpfile, index_col=0)\n",
    "df_1000G_genotypes_alleles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute MAF by Population (1000 Genomes data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use both `plink` and `pandas` for this computation. `pandas` to make the `.fam` files\n",
    "with either population or subpopulation names in the family field. `plink` to\n",
    "compute the frequencies using each of the `.fam` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# == NOTE ==\n",
    "# This was used to generate different .fam files (with population and superpopulation data)\n",
    "# so that plink could generate frequencies relative to those groups.\n",
    "# It's already done now, results are read in the next cell.\n",
    "\n",
    "# basedir = \"/home/juan/tesina/1000Genomes_data/galanter-extracted-SNPs-from-1000Genomes/\"\n",
    "# fam_file_fields = [\"fam\", \"sample\", \"father\", \"mother\", \"sex\", \"phenotype\"]\n",
    "\n",
    "# for panel_name in [\"gal_completo\", \"gal_affy\"]:\n",
    "\n",
    "#     # Read original .fam with no populations info\n",
    "#     fn = basedir + \"{}.fam\".format(panel_name)\n",
    "#     nofam = pd.read_csv(fn, engine=\"python\", sep=\"\\s*\", names=fam_file_fields,\n",
    "#                         index_col=\"sample\")\n",
    "\n",
    "#     # Write the populations info to a new .tfam\n",
    "#     df = nofam.copy()\n",
    "#     df[\"fam\"] = df_1000G_sample_populations[\"population\"]\n",
    "#     df.reset_index(inplace=True)\n",
    "#     df = df[fam_file_fields]\n",
    "#     fn = basedir + \"{}.populations.fam\".format(panel_name)\n",
    "#     df.to_csv(fn, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "#     # Write the superpopulations info to a new .tfam\n",
    "#     df = nofam.copy()\n",
    "#     df[\"fam\"] = df_1000G_sample_populations[\"super_population\"]\n",
    "#     df.reset_index(inplace=True)\n",
    "#     df = df[fam_file_fields]\n",
    "#     fn = basedir + \"{}.superpopulations.fam\".format(panel_name)\n",
    "#     df.to_csv(fn, sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the previous cell (`.fam` files generation) and the next cell (reading of the `.frq.strat` files),\n",
    "plink was run to compute the frequencies per population.\n",
    "\n",
    "Check the bash script `/home/juan/tesina/1000Genomes_data/galanter-extracted-SNPs-from-1000Genomes/2_compute_freqs.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = \"/home/juan/tesina/1000Genomes_data/galanter-extracted-SNPs-from-1000Genomes\"\n",
    "\n",
    "mafs = {\"per_population\": {}, \"per_superpopulation\": {}}\n",
    "\n",
    "for panel_label in panel_labels:\n",
    "    fn = \"{}/{}.populations.frq.strat\".format(basedir, panel_label.lower())\n",
    "    df = pd.read_csv(fn, engine=\"python\", sep=\"\\s*\")\n",
    "    df = df.pivot_table(values=\"MAF\", index=\"SNP\", columns=\"CLST\")\n",
    "    df = df.applymap(lambda freq: 1 - freq if freq > 0.5 else freq)\n",
    "    mafs[\"per_population\"][panel_label] = df\n",
    "\n",
    "    fn = \"{}/{}.superpopulations.frq.strat\".format(basedir, panel_label.lower())\n",
    "    df = pd.read_csv(fn, engine=\"python\", sep=\"\\s*\")\n",
    "    df = df.pivot_table(values=\"MAF\", index=\"SNP\", columns=\"CLST\")\n",
    "    df = df.applymap(lambda freq: 1 - freq if freq > 0.5 else freq)\n",
    "    mafs[\"per_superpopulation\"][panel_label] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## == NOTE ==\n",
    "## This was how I previously computed --manually-- the MAFs per population\n",
    "## PLINK already does it, so this code is a fossil. I kept it just in case for now.\n",
    "\n",
    "# def maf_by_population_from_genotypes(df_genotypes, df_populations, dumpfile):\n",
    "\n",
    "#     if not isfile(dumpfile):\n",
    "#         freq_by_population = defaultdict(dict)\n",
    "\n",
    "#         for rs, genotypes in df_genotypes.iteritems():\n",
    "#             ref_alleles_count = defaultdict(lambda: 0)\n",
    "#             total_alleles_count = defaultdict(lambda: 0)\n",
    "#             ref = df_genotypes.iloc[1, 1][0]  # Arbitrarily pick a ref allele for the count\n",
    "\n",
    "#             for sample, genotype in genotypes.iteritems():\n",
    "#                 # genotype is a 'TT' or 'TC' like string, transform it to a number of ref_alleles\n",
    "#                 ref_count = genotype.count(ref)\n",
    "#                 try:\n",
    "#                     population = df_populations.loc[sample].population\n",
    "#                     super_population = df_populations.loc[sample].super_population\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "#                 ref_alleles_count[population] += ref_count\n",
    "#                 ref_alleles_count[super_population] += ref_count\n",
    "#                 total_alleles_count[population] += 2\n",
    "#                 total_alleles_count[super_population] += 2\n",
    "\n",
    "#             for population, ref_alleles in ref_alleles_count.items():\n",
    "#                 freq_by_population[rs][population] = ref_alleles / total_alleles_count[population]\n",
    "\n",
    "#         freqs = pd.DataFrame(dict(freq_by_population)).transpose()\n",
    "#         mafs = freqs.applymap(lambda freq: min(freq, 1 - freq))\n",
    "#         mafs.to_csv(dumpfile)\n",
    "\n",
    "#     return pd.read_csv(dumpfile, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !rm dumpfiles/1000G_MAF_per_population.csv  # Flush cache ;)\n",
    "\n",
    "# maf_1000G = maf_by_population_from_genotypes(\n",
    "#     df_1000G_genotypes_alleles, df_1000G_sample_populations,\n",
    "#     \"dumpfiles/1000G_MAF_per_population.csv\"\n",
    "# )\n",
    "# maf_1000G.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparar SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"1000 Genomas:\", len(df_1000G_SNPs))\n",
    "print(\"Galanter panel:\", len(galanter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA** sobre los SNPs que faltan: en el proceso de leer la data de 1000 genomas estoy dejando afuera tres SNPs, porque tienen más de 2 variantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_in_1000g = set(galanter.index) - set(df_1000G_SNPs.index)\n",
    "galanter.loc[missing_in_1000g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear listas de SNPs ya filtradas para GAL total y GAL parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1000G_SNPs.index.values.tofile(\n",
    "    \"/home/juan/tesina/admixture/galT.snps\", sep=\"\\n\", format=\"%s\"\n",
    ")\n",
    "df_1000G_SNPs.index.intersection(present.index).values.tofile(\n",
    "    \"/home/juan/tesina/admixture/galP.snps\", sep=\"\\n\", format=\"%s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minidescribe(df):\n",
    "    df = df.join(mafs[\"per_population\"][\"GAL_Completo\"]).loc[:, \"ACB\":\"YRI\"]\n",
    "    df = df.describe().loc[[\"mean\", \"std\"]].T\n",
    "    df = df.applymap(lambda x: round(x, 2))\n",
    "    df[\"mean ± std\"] = df[\"mean\"].astype(str) + \" ± \" + df[\"std\"].astype(str)\n",
    "    return df\n",
    "\n",
    "galanter_summary = minidescribe(galanter)\n",
    "present_summary = minidescribe(present)\n",
    "\n",
    "# This uses Galanter-provided MAFs!\n",
    "maf_mean_comparison = galanter_summary.join(present_summary, rsuffix=\"_\")\n",
    "maf_mean_comparison[\"Difference\"] = maf_mean_comparison[\"mean\"] - maf_mean_comparison[\"mean_\"]\n",
    "maf_mean_comparison.drop([\"mean\", \"std\", \"mean_\", \"std_\"], axis=1, inplace=True)\n",
    "maf_mean_comparison.columns = [\"GAL_Completo\", \"GAL_Affy\", \"Difference\"]\n",
    "\n",
    "print(\"Comparación usando los MAFs provistos **por Galanter**\")\n",
    "maf_mean_comparison.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Entre ambos paneles no hay ninguna población cuyos MAFs (promediados para todos los AIMs!) cambien demasiado. Es un comienzo.\n",
    "\n",
    "Acá uso los MAFs provistos por el csv de Galanter, no los calculados con 1000 Genomas + plink.\n",
    "\n",
    "A continuación, uso los de 1000 Genomas, por población."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nrows = 1\n",
    "ncols = 2\n",
    "plot_width = 4\n",
    "plot_height = 4\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, sharey=True)\n",
    "fig.set_figwidth(ncols * plot_width)\n",
    "fig.set_figheight(nrows * plot_height)\n",
    "\n",
    "for i, panel_name in enumerate([\"GAL_Completo\", \"GAL_Affy\"]):\n",
    "    \n",
    "    plot_title = \"MAFs promedio en {}\".format(panel_name)    \n",
    "    df = mafs[plot_name][panel_name][['AFR', 'EUR', 'AMR']]\n",
    "    ax = axes[i]\n",
    "\n",
    "    rot = 90 if len(populations_to_plot) > 5 else 0\n",
    "    bp = df.boxplot(ax=ax, rot=rot, patch_artist=True, return_type=\"dict\",\n",
    "                    showfliers=False, showcaps=False)\n",
    "    \n",
    "    ax.set_title(plot_title, y=1.2, fontsize=13)\n",
    "    \n",
    "    for patch in bp[\"boxes\"]:\n",
    "        patch.set_facecolor(\"LightSkyBlue\")\n",
    "        patch.set_edgecolor(\"white\")\n",
    "    for whisker in bp[\"whiskers\"]:\n",
    "        whisker.set_color(\"MidnightBlue\")\n",
    "    for xlabel in ax.get_xticklabels():\n",
    "        xlabel.set_color(\"#666666\")\n",
    "\n",
    "    ax.xaxis.grid()\n",
    "    hide_spines_and_ticks(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter.filter([\"NAM_AF\", \"EUR_AF\", \"AFR_AF\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Should mean continental MAFs be similar to the ones in Galanter csv? They are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% run helpers/text_helpers.py\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "populations_to_plot = [\"PEL\", \"CLM\", \"MXL\", \"PUR\",\n",
    "                       \"IBS\", \"CEU\", \"YRI\", \"LWK\",\n",
    "                       \"CHB\", \"GIH\"]\n",
    "\n",
    "plot_width = 5\n",
    "plot_height = 4\n",
    "ncols = 5\n",
    "nrows = ceil(len(populations_to_plot) / ncols)\n",
    "\n",
    "pops_matrix = np.array(populations_to_plot)\n",
    "pops_matrix.resize(nrows, ncols)\n",
    "\n",
    "figsize = (ncols * plot_width, nrows * plot_height)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, sharey=\"row\", figsize=figsize)\n",
    "\n",
    "for row in np.arange(nrows):\n",
    "    for col in np.arange(ncols):\n",
    "        population = pops_matrix[row][col]\n",
    "\n",
    "        mafs_df = mafs[\"per_population\"]\n",
    "        ax = axes[row][col]\n",
    "        \n",
    "        if population == 0:  # Hack to deal with the np.array#resize extra values\n",
    "            continue\n",
    "            \n",
    "        df = pd.DataFrame({\"GAL_Affy\": mafs_df[\"GAL_Affy\"][population],\n",
    "                           \"GAL_Completo\": mafs_df[\"GAL_Completo\"][population]})\n",
    "        pop_description = df_1000G_population_names.loc[population]['Population Description']\n",
    "        ax.set_title(population + \"\\n\" + trunc_text(pop_description, 35), y=1.1, fontsize=15)\n",
    "        bp = df.boxplot(ax=ax, return_type='dict', patch_artist=True,\n",
    "                        showcaps=False, showfliers=False)\n",
    "\n",
    "        for patch in bp[\"boxes\"]:\n",
    "            patch.set_facecolor(\"SkyBlue\")\n",
    "            patch.set_edgecolor(\"white\")\n",
    "        for whiskers in bp[\"whiskers\"]:\n",
    "            whiskers.set_color(\"MidnightBlue\")\n",
    "\n",
    "        ax.set_ylim([0, 0.5])\n",
    "        hide_spines_and_ticks(ax)\n",
    "        ax.xaxis.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_frequencies = lambda panel_name: mafs[\"per_population\"][panel_name].mean()\n",
    "std_frequencies = lambda panel_name: mafs[\"per_population\"][panel_name].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "df = pd.DataFrame(OrderedDict([\n",
    "    (\"GAL_Completo\", mean_frequencies(\"GAL_Completo\")),\n",
    "    (\"GAL_Affy\", mean_frequencies(\"GAL_Affy\")),\n",
    "]))\n",
    "\n",
    "df[\"|Freq Diff|\"] = (df[\"GAL_Completo\"] - df[\"GAL_Affy\"]).apply(abs)\n",
    "\n",
    "df = df.applymap(lambda n: round(n, 2))\n",
    "freq_diff = df.join(df_1000G_population_names[['Population Description', 'Super Population Code']]).fillna('')\n",
    "freq_diff.sort_values(by='|Freq Diff|', ascending=False, inplace=True)\n",
    "\n",
    "freq_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_diff.groupby(\"Super Population Code\").mean().applymap(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Preparar datos para PCA y ADMIXTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "panel_indices = OrderedDict()\n",
    "panel_indices[\"GAL Total\\n443 SNPs\"] = galanter.index\n",
    "panel_indices[\"GAL Parcial\\n170 SNPs\"] = present.index\n",
    "panel_indices[\"Panel Aleatorio x 1\\n432 SNPs\"] = control_genotypes[\"1\"].columns\n",
    "panel_indices[\"Panel Aleatorio x 10\\n4.078 SNPs\"] = control_genotypes[\"10\"].columns\n",
    "panel_indices[\"Panel Aleatorio x 100\\n40.728 SNPs\"] = control_genotypes[\"100\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "indices_per_panel = OrderedDict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "plot_colors = yaml.load(open(\"data/plot_colors.yml\", \"r\"))\n",
    "plot_markers = yaml.load(open(\"data/plot_markers.yml\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparar datasets de muestras elegidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "samples = df_1000G_sample_populations\n",
    "\n",
    "def merge_dicts(*dict_args):\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result\n",
    "\n",
    "def indices(pop_dict):  \n",
    "    populations = []\n",
    "    for label, pop_list in pop_dict.items():\n",
    "        mask = samples.population.isin(pop_list)\n",
    "        populations.append(samples[mask])\n",
    "\n",
    "    return pd.concat(populations).index\n",
    "\n",
    "datasets_general = OrderedDict()\n",
    "datasets_by_pop = OrderedDict()\n",
    "\n",
    "all_latinos = {'latinos': ['PEL', 'MXL', 'CLM', 'PUR']}\n",
    "e = {'europeans': ['IBS', 'TSI']}\n",
    "ea = merge_dicts(e, {'africans': ['YRI', 'LWK']})\n",
    "eac = merge_dicts(ea, {'east_asians': ['CHB', 'CHS']})\n",
    "eaci = merge_dicts(eac, {'south_asians': ['GIH']})\n",
    "\n",
    "datasets_general['Latinos'] = all_latinos\n",
    "datasets_general['Latinos, Europeos'] = merge_dicts(e, all_latinos)\n",
    "datasets_general['Latinos, Europeos, Africanos'] = merge_dicts(ea, all_latinos)\n",
    "datasets_general['Latinos, Europeos, Africanos, Chinos'] = merge_dicts(eac, all_latinos)\n",
    "datasets_general['Latinos, Europeos, Africanos, Chinos, Indios'] = merge_dicts(eaci, all_latinos)\n",
    "\n",
    "# Transform population codes into dataframe indices (sample IDs)\n",
    "# And write the sample IDs list to a file with the dataset name\n",
    "for dataset_group in [datasets_general, datasets_by_pop]:\n",
    "    for label, pop_dict in dataset_group.items():\n",
    "        dataset_group[label] = indices(pop_dict)\n",
    "        filename = \"_\".join(label.replace(\"\\n\", \" \").split(\", \")).lower()\n",
    "        dataset_group[label].values.tofile(\"/home/juan/tesina/dataset_dumps/\" + filename,\n",
    "                                           sep=\"\\n\", format=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Memory hog! Will eat ~2Gb RAM if control_3_genotypes is combined.\n",
    "all_controls = control_genotypes.combine_first(control_2_genotypes).combine_first(control_3_genotypes)\n",
    "all_controls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run plot_PCAs.py\n",
    "\n",
    "RUN_PCA = False\n",
    "\n",
    "all_genotypes = df_1000G_genotypes.combine_first(all_controls)\n",
    "\n",
    "for dataset_label, indices in datasets_general.items():\n",
    "    if not RUN_PCA:\n",
    "        continue\n",
    "\n",
    "    genotypes = all_genotypes.loc[indices, :]\n",
    "    \n",
    "    pca = plot_PCAs(dataset_label, panel_indices, genotypes,\n",
    "                    df_1000G_sample_populations,\n",
    "                    plot_markers, plot_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr ADMIXTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# panels_indices = {\n",
    "#     \"galT\": galanter.index,\n",
    "#     \"galP\": present.index,\n",
    "#     \"controlx1\": control_genotypes.columns,\n",
    "#     \"controlx10\": control_2_genotypes.columns,\n",
    "#     \"controlx100\": control_3_genotypes.columns,\n",
    "# }\n",
    "\n",
    "# for panel_tag, panel_indices in panels_.items():\n",
    "#     df_1000G_SNPs.index.intersection(panel_indices).values.tofile(\n",
    "#         \"~/tesina/admixture/{}.snps\".format(panel_tag), sep=\"\\n\", format=\"%s\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En `~/tesina/admixture/` están los archivos `*.samples` con los sample IDs de 1000Genomas de cada dataset (`L`, `LE`, `LEA` ...).\n",
    "* En el mismo directorio copié los bfiles (`.bed`, `.bim` ...) de PLINK de cada panel (`galT`, `galP`, `controlx1`, `controlx10`, `controlx100`)\n",
    "* Ahí mismo corrí `./create_datasets_run_admixture.sh <nombre-de-panel> <nombre-de-panel-2> ..`\n",
    "* -> Se crean automáticamente directorios por cada combineta de dataset + panel con los archivos `*.P`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer resultados de ADMIXTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CV Errors for different K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Generar el `CV_error_summary` que leo para los gráficos siguientes:\n",
    "\n",
    "cd /home/juan/tesina/admixture\n",
    "grep CV */*.log > CV_error_summary\n",
    "cat CV_error_summary | sed s'#[/:)]#,#g' | sed s'/CV error (K=//g' | awk -F, '{OFS=\",\"; print $1, $3, $5}' | sed s'/, /,/g' > CV_error_summary.clean\n",
    "cd /home/juan/tesina/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_names = {\n",
    "    \"L\": \"Latinos\",\n",
    "    \"LE\": \"Latinos, Europeos\",\n",
    "    \"LEA\": \"Latinos, Europeos, Africanos\",\n",
    "    \"LEAC\": \"Latinos, Europeos,\\nAfricanos, Chinos\",\n",
    "    \"LEACI\": \"Latinos, Europeos,\\nAfricanos, Chinos, Indios\"\n",
    "}\n",
    "\n",
    "panel_names = {\n",
    "    \"galP\": \"GAL Parcial (170 SNPs)\",\n",
    "    \"galT\": \"GAL Total (443 SNPs)\",\n",
    "    \"controlx1\": \"Panel Aleatorio x 1 (432 SNPs)\",\n",
    "    \"controlx10\": \"Panel Aleatorio x 10 (4.078 SNPs)\",\n",
    "    \"controlx100\": \"Panel Aleatorio x 100 (40.728 SNPs)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "plot_panel_colors = {\n",
    "    \"galP\": \"Tomato\",\n",
    "    \"galT\": \"CornflowerBlue\",\n",
    "#     \"controlx1\": \"#555555\",\n",
    "#     \"controlx10\": \"#666666\",\n",
    "#     \"controlx100\": \"#777777\",\n",
    "}\n",
    "\n",
    "cv_errors = pd.read_csv(\"~/tesina/admixture/CV_error_summary.clean\",\n",
    "                        names=['dataset_panel', 'K', 'CV_error'])\n",
    "\n",
    "cv_errors['dataset'] = cv_errors['dataset_panel'].apply(lambda x: x.split(\"_\")[0])\n",
    "cv_errors['panel'] = cv_errors['dataset_panel'].apply(lambda x: x.split(\"_\")[1])\n",
    "cv_errors = cv_errors.drop('dataset_panel', axis=1)\n",
    "cv_errors = cv_errors.set_index(['dataset', 'panel', 'K']).sort_index()\n",
    "\n",
    "\n",
    "Ks = cv_errors.index.get_level_values('K').unique()\n",
    "\n",
    "width, height = (5, 3)\n",
    "cols, rows = (2, 3)\n",
    "fig = plt.figure(figsize=(width * cols, height * rows))\n",
    "axes = list(np.arange(cols * rows) + 1)\n",
    "axes.reverse()\n",
    "\n",
    "datasets = cv_errors.index.get_level_values('dataset').unique()\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ax = plt.subplot(rows, cols, axes.pop())\n",
    "    lines = []\n",
    "    \n",
    "    panels = cv_errors.loc[dataset].index.get_level_values('panel').unique()\n",
    "    panels = [panel for panel in panels if \"gal\" in panel]  # Filter out the control panels for this chart\n",
    "    for panel in panels:\n",
    "        data = cv_errors.loc[(dataset, panel)]            \n",
    "        data.plot(ax=ax, marker=\".\", color=plot_panel_colors[panel], zorder=1)\n",
    "        \n",
    "        x_min = data['CV_error'].idxmin()\n",
    "        y_min = data['CV_error'].min()\n",
    "        min_marker = ax.scatter(x_min, y_min, marker=\"v\", color=\"DarkGreen\", zorder=2, s=35)\n",
    "        \n",
    "        # TODO: Add a special mark at the minimum y value\n",
    "    \n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    panel_labels = [panel_names[panel] for panel in panels]\n",
    "    ax.set_title(\"Dataset: \" + dataset_names[dataset], fontsize=12)\n",
    "    ax.set_ylabel(\"CV Error\", fontsize=11)\n",
    "    ax.set_xlabel(\"K\", fontsize=11)\n",
    "    ax.legend_.remove()\n",
    "\n",
    "# Ugly hack to get the legend in a separate subplot slot\n",
    "ax = plt.subplot(rows, cols, axes.pop())\n",
    "ax.legend(lines + [min_marker], panel_labels + ['Valor óptimo de K'],\n",
    "          fontsize=13, loc='upper left', fancybox=True, scatterpoints=1)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "for loc in ['top', 'bottom', 'left', 'right']:\n",
    "    ax.spines[loc].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ADMIXTURE ancestry ratios per individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dataset_Ks = OrderedDict([(\"L\", 3), (\"LE\", 3), (\"LEA\", 3), (\"LEAC\", 4), (\"LEACI\", 5)])\n",
    "panels = [\"galT\", \"galP\", \"controlx1\", \"controlx10\", \"controlx100\"]\n",
    "admixture_results = defaultdict(OrderedDict)\n",
    "ancestral_components = {0: 'EUR', 1: 'NAM', 2: 'AFR', 3: 'EAS', 4: 'SAS'}\n",
    "\n",
    "population_groups = OrderedDict([\n",
    "    ('Latinos',   ['CLM', 'MXL', 'PEL', 'PUR']),\n",
    "    ('Europeos',  ['TSI', 'IBS']),\n",
    "    ('Africanos', ['LWK', 'YRI']),\n",
    "    ('Chinos',    ['CHS', 'CHB']),\n",
    "    ('Indios',    ['GIH'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset_tag, optimal_K in dataset_Ks.items():\n",
    "    for panel_tag in panels:\n",
    "        fdir = \"~/tesina/admixture/{}_{}/\".format(dataset_tag, panel_tag)\n",
    "        filename = \"{}_{}.{}.Q\".format(dataset_tag, panel_tag, optimal_K)\n",
    "        samples_filename = \"{}_{}.fam\".format(dataset_tag, panel_tag)\n",
    "        ancestry_values = pd.read_csv(fdir + filename, sep=\"\\s+\",\n",
    "                                      names=list(range(optimal_K)))\n",
    "        sample_ids = pd.read_csv(fdir + samples_filename, sep=\"\\s+\",\n",
    "                                 index_col=0, usecols=[0], names=['sample_id'])\n",
    "        ancestry_values['sample_id'] = sample_ids.index\n",
    "        populations = df_1000G_sample_populations.loc[sample_ids.index]['population']\n",
    "        ancestry_values['population'] = populations.values\n",
    "        ancestry_values['super_population'] = [df_1000G_population_names.loc[population]['Super Population Code']\n",
    "                                               for population in populations.values]\n",
    "\n",
    "        # Reorder ancestry columns to always have the same ancestral populations\n",
    "        # in the same positions (and then the same order and colors in the plots).\n",
    "        # Desired order: European, Amerindian, African, Chinese, Indian\n",
    "        # This is necessarily hardcoded after visual inspection of the plots.\n",
    "        fixed_columns = ['sample_id', 'population', 'super_population']\n",
    "        if panel_tag == \"galT\":\n",
    "            if dataset_tag == \"L\":\n",
    "                ancestry_values = ancestry_values[[2, 0, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LE\":\n",
    "                ancestry_values = ancestry_values[[2, 0, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LEA\":\n",
    "                ancestry_values = ancestry_values[[2, 1, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEAC\":\n",
    "                ancestry_values = ancestry_values[[1, 3, 2, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEACI\":\n",
    "                ancestry_values = ancestry_values[[3, 0, 2, 1, 4] + fixed_columns]\n",
    "        if panel_tag == \"galP\":\n",
    "            if dataset_tag == \"L\":\n",
    "                ancestry_values = ancestry_values[[2, 0, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LE\":\n",
    "                ancestry_values = ancestry_values[[2, 1, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEA\":\n",
    "                ancestry_values = ancestry_values[[1, 2, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEAC\":\n",
    "                ancestry_values = ancestry_values[[3, 1, 0, 2] + fixed_columns]\n",
    "            if dataset_tag == \"LEACI\":\n",
    "                ancestry_values = ancestry_values[[3, 0, 2, 1, 4] + fixed_columns]\n",
    "        if panel_tag == \"controlx1\":\n",
    "            if dataset_tag == \"L\":\n",
    "                ancestry_values = ancestry_values[[1, 2, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LE\":\n",
    "                ancestry_values = ancestry_values[[1, 0, 2] + fixed_columns]\n",
    "            if dataset_tag == \"LEA\":\n",
    "                ancestry_values = ancestry_values[[1, 2, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEAC\":\n",
    "                ancestry_values = ancestry_values[[3, 0, 2, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LEACI\":\n",
    "                ancestry_values = ancestry_values[[4, 0, 2, 3, 1] + fixed_columns]\n",
    "        if panel_tag == \"controlx10\":\n",
    "            if dataset_tag == \"L\":\n",
    "                ancestry_values = ancestry_values[[2, 0, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LE\":\n",
    "                ancestry_values = ancestry_values[[2, 0, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LEA\":\n",
    "                ancestry_values = ancestry_values[[2, 1, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEAC\":\n",
    "                ancestry_values = ancestry_values[[3, 0, 2, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LEACI\":\n",
    "                ancestry_values = ancestry_values[[4, 0, 2, 3, 1] + fixed_columns]\n",
    "        if panel_tag == \"controlx100\":\n",
    "            if dataset_tag == \"L\":\n",
    "                ancestry_values = ancestry_values[[2, 0, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LE\":\n",
    "                ancestry_values = ancestry_values[[1, 0, 2] + fixed_columns]\n",
    "            if dataset_tag == \"LEA\":\n",
    "                ancestry_values = ancestry_values[[2, 1, 0] + fixed_columns]\n",
    "            if dataset_tag == \"LEAC\":\n",
    "                ancestry_values = ancestry_values[[3, 0, 2, 1] + fixed_columns]\n",
    "            if dataset_tag == \"LEACI\":\n",
    "                ancestry_values = ancestry_values[[4, 0, 2, 3, 1] + fixed_columns]\n",
    "                \n",
    "        # Reassign the column numbers\n",
    "        columns_renumbered = list(range(len(ancestry_values.columns) - len(fixed_columns)))\n",
    "        ancestry_values.columns = (columns_renumbered + fixed_columns)\n",
    "        \n",
    "        # Replace the numbers with the continental population codes\n",
    "        # This naming of the ancestry columns only makes sense with the previous ordering,\n",
    "        # which placed the components in the same order as the one in \"ancestral_components\"\n",
    "        renamed_columns = [ancestral_components[n] for n in ancestry_values.columns\n",
    "                          if n not in fixed_columns]\n",
    "        ancestry_values.columns = renamed_columns + fixed_columns\n",
    "        \n",
    "        # Reorder the named columns\n",
    "        ancestral_components_ordered = list(ancestral_components.values())[:len(renamed_columns)]\n",
    "        ancestry_values = ancestry_values[ancestral_components_ordered + fixed_columns]\n",
    "\n",
    "        # Sort the samples\n",
    "        ancestry_values = ancestry_values.sort_values(['super_population', 'population', 'EUR'])\n",
    "        ancestry_values = ancestry_values.reset_index(drop=True)\n",
    "\n",
    "        admixture_results[dataset_tag][panel_tag] = ancestry_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def config_admixture_plot(ax, df, ylabel_on=True):\n",
    "    ax.set_title(panel_name, fontsize=14, position=(0.5, 1.4))\n",
    "    \n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    \n",
    "    # Set population names on the xticks\n",
    "    indices_by_population = [df[df['population'] == population].index.values\n",
    "                             for population in df['population'].unique()]\n",
    "    xtick_positions = [np.median(indices) for indices in indices_by_population]\n",
    "    ax.set_xticks(xtick_positions)\n",
    "    ax.set_xticklabels(df['population'].unique(), rotation=0)\n",
    "    ax.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "    # Separate the populations with a line\n",
    "    \n",
    "    separation_lines = [max(indices) for indices in indices_by_population]\n",
    "    [plt.axvline(x=x, color=\"k\", lw=1) for x in separation_lines[:-1]]\n",
    "\n",
    "    if ylabel_on:\n",
    "        ax.set_ylabel(\"Proporción de\\nancestrías\", fontsize=11)\n",
    "        \n",
    "    ax.set_ylim((0, 1))\n",
    "    ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "    return ax\n",
    "\n",
    "for dataset_tag, panel_results in admixture_results.items():\n",
    "    K = dataset_Ks[dataset_tag]\n",
    "    dataset_name = dataset_names[dataset_tag].replace(\"\\n\", \" \")\n",
    "        \n",
    "    # One figure per panel per dataset on top\n",
    "    \n",
    "    rows = len(panels)\n",
    "    fig = plt.figure(figsize=(12, 2 * rows))\n",
    "    axes = (np.arange(len(panels)) + 1).tolist()[::-1]\n",
    "    \n",
    "    for panel_tag, df in panel_results.items():\n",
    "        panel_name = panel_names[panel_tag]\n",
    "        ax = plt.subplot(rows, 1, axes.pop())\n",
    "        df.plot(ax=ax, kind=\"bar\", stacked=True, width=1, linewidth=0)\n",
    "        ax = config_admixture_plot(ax, df)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    title = \"Dataset: {} ($K={}$)\".format(dataset_name, K)\n",
    "    fig.suptitle(title, fontsize=17)\n",
    "    plt.subplots_adjust(top=(0.75))  # Make space for the fig.suptitle\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#     # Detail plots for each population group\n",
    "    \n",
    "#     # Detail plots dimensions and subplot number generation\n",
    "#     width, height = (6, 2)\n",
    "    \n",
    "#     # Hacky! Dataset tags are acronyms of the population groups\n",
    "#     # So I'm inferring the number of population group from the length of the tag.\n",
    "#     cols, rows = (2, len(dataset_tag))  # One row of plots per population group\n",
    "    \n",
    "#     if dataset_tag == \"L\":\n",
    "#         rows = len(population_groups[\"Latinos\"])\n",
    "    \n",
    "#     fig = plt.figure(figsize=(width * cols, height * rows))\n",
    "\n",
    "#     odd_subplots = [n for n in np.arange(rows * cols) + 1 if n % 2 != 0]\n",
    "#     even_subplots = list(np.array(odd_subplots) + 1)\n",
    "#     axes = {\"galT\": odd_subplots[::-1], \"galP\": even_subplots[::-1]}\n",
    "\n",
    "#     sorted_index = []  # Hack. See (1) below.\n",
    "\n",
    "#     # One subplot per population group per panel in the dataset\n",
    "#     for panel_tag, df in panel_results.items():\n",
    "#         panel_name = panel_names[panel_tag]\n",
    "        \n",
    "#         if dataset_tag == \"L\":\n",
    "#             # Hack to get one plot per latino population in this dataset\n",
    "#             pg = dict([(p, [p]) for p in population_groups[\"Latinos\"]])\n",
    "#         else:\n",
    "#             pg = population_groups\n",
    "            \n",
    "#         for population_group_name, population_group in pg.items():\n",
    "#             mask = df['population'].isin(population_group)\n",
    "#             df_pop = df[mask].dropna()\n",
    "#             if len(df_pop) == 0:\n",
    "#                 continue\n",
    "    \n",
    "#             # (1) Hack to get the same sample order in both columns of plots\n",
    "#             if panel_tag == \"galT\":\n",
    "#                 sorted_index = [df_pop.index] + sorted_index\n",
    "#             elif panel_tag == \"galP\":\n",
    "#                 df_pop = df_pop.loc[sorted_index.pop()]\n",
    "\n",
    "#             # I need consecutive indices for the xtick labels position calculation later:\n",
    "#             df_pop = df_pop.reset_index(drop=True)\n",
    "            \n",
    "#             ax_id = axes[panel_tag].pop()\n",
    "#             ax = plt.subplot(rows, cols, ax_id)\n",
    "#             df_pop.plot(ax=ax, kind=\"bar\", stacked=True, width=1, linewidth=0)\n",
    "            \n",
    "#             ylabel_on = (ax_id % 2 != 0)  # Y-axis label only on plots on the left (odd plots)\n",
    "#             title_on = (ax_id in [1, 2])  # Only top plots carry the panel_name as title\n",
    "\n",
    "#             ax = config_admixture_plot(ax, df_pop, ylabel_on=ylabel_on)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_superpop_from_poptag(tag):\n",
    "    ref = df_1000G_population_names\n",
    "\n",
    "    if tag not in ref.index:\n",
    "        return tag\n",
    "    \n",
    "    return ref.loc[tag]['Super Population Code']\n",
    "\n",
    "def config_admixture_mean_plot(ax, df, title=None, ylabel_on=True):\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=12.5, fontweight=\"bold\")\n",
    "\n",
    "#     if ylabel_on:\n",
    "#         ax.set_ylabel(\"Proporción de\\nancestrías\", fontsize=11)\n",
    "\n",
    "    ax.set_ylim((-0.05, 1.05))\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    xticklabels = plt.xticks()[1]\n",
    "    xticks_rot = 0 if len(xticklabels) < 10 else 45\n",
    "    ax.set_xticklabels(xticklabels, rotation=xticks_rot)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_frame_on(False)\n",
    "    ax.legend_.remove()\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "for dataset_tag, panel_results in admixture_results.items():\n",
    "    K = dataset_Ks[dataset_tag]\n",
    "    dataset_name = dataset_names[dataset_tag].replace(\"\\n\", \" \")\n",
    "    \n",
    "    rows, cols = len(panel_results.keys()), 1\n",
    "    fig = plt.figure(figsize=(cols * width, rows * height))\n",
    "    axes = (np.arange(rows * cols) + 1).tolist()[::-1]\n",
    "    \n",
    "    for panel_tag, df in panel_results.items():\n",
    "        \n",
    "        panel_name = panel_names[panel_tag]\n",
    "        ax_id = axes.pop()\n",
    "        ax = plt.subplot(rows, cols, ax_id)\n",
    "        \n",
    "        by_population = df.groupby(\"population\").mean()\n",
    "        # by_superpopulation = df.groupby(\"super_population\").mean()\n",
    "        # both = pd.concat([by_population, by_superpopulation])\n",
    "\n",
    "        by_population[\"superpop\"] = [get_superpop_from_poptag(ix)\n",
    "                                     for ix in by_population.index.values]\n",
    "        by_population = by_population.sort_values([\"superpop\", \"EUR\"])\n",
    "        \n",
    "        by_population.plot(ax=ax, kind=\"bar\", stacked=True, width=1, lw=0.35, rot=0)\n",
    "        config_admixture_mean_plot(ax, by_population, title=panel_name, ylabel_on=True)\n",
    "    \n",
    "    figtitle = \"{} (K={})\".format(dataset_name, K)\n",
    "    fig.suptitle(figtitle, fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=(0.90))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Triangular plot or 3D plot with the K=3 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ternary\n",
    "\n",
    "\n",
    "for dataset_tag, panel_results in admixture_results.items():\n",
    "    if len(dataset_tag) > 3:\n",
    "        continue\n",
    "    \n",
    "    K = dataset_Ks[dataset_tag]\n",
    "    dataset_name = dataset_names[dataset_tag].replace(\"\\n\", \" \")\n",
    "    \n",
    "    rows, cols = 1, len(panels)\n",
    "    width, height = 10, 8\n",
    "    fig = plt.figure(figsize=(cols * width, rows * height))\n",
    "\n",
    "    axes = (np.arange(rows * cols) + 1).tolist()[::-1]\n",
    "    \n",
    "    for panel_tag, df in panel_results.items():\n",
    "        panel_name = panel_names[panel_tag]\n",
    "            \n",
    "        ax_id = axes.pop()\n",
    "        ax = plt.subplot(rows, cols, ax_id)\n",
    "        ax.axis(\"off\")\n",
    "        fig, tax = ternary.figure(scale=1, ax=ax)\n",
    "\n",
    "        for population in df['population'].unique():\n",
    "            mask = df['population'] == population\n",
    "            tax.scatter(df[mask].iloc[:, :3].as_matrix(), label=population,\n",
    "                        color=plot_colors[population], marker=plot_markers[population])\n",
    "\n",
    "        tax.set_title(\"Dataset: {}\\n{}\".format(dataset_name, panel_name),\n",
    "                      position=(0.5, 1.05))\n",
    "\n",
    "        tax.boundary(linewidth=1)\n",
    "\n",
    "        fontsize = 15\n",
    "        tax.left_axis_label(df.columns[2], fontsize=fontsize)\n",
    "        tax.bottom_axis_label(df.columns[0], fontsize=fontsize)\n",
    "        tax.right_axis_label(df.columns[1], fontsize=fontsize)\n",
    "\n",
    "        tax.ticks(axis=\"lbr\", linewidth=1, multiple=0.1)\n",
    "        tax.gridlines(multiple=0.1)\n",
    "        tax.legend(frameon=False, fontsize=12, scatterpoints=1)\n",
    "\n",
    "        tax.clear_matplotlib_ticks()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read HDGP markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para descargar los datos de HGDP usé ftp_download_HGDP.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEPH dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_1_HGDP-CEPH_v3/hgdp-ceph-marker.out\"\n",
    "hgdp_ceph_markers = pd.read_csv(fn, sep=\"\\t\")\n",
    "hgdp_ceph_markers = hgdp_ceph_markers.rename(columns={\"chrom\": \"chr\", \"physical_pos\": \"pos\"})\n",
    "mask = hgdp_ceph_markers[\"type_marker\"].isin([\"SNP\", \"snp\"])\n",
    "hgdp_ceph_markers = hgdp_ceph_markers[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_1_HGDP-CEPH_v3/hgdp-ceph-geno.out\"\n",
    "# Filter only biallelic SNPs!\n",
    "hgdp_ceph_genotypes = pd.read_csv(fn, sep=\"\\t\", index_col=\"hgdp_id\")\n",
    "hgdp_ceph_genotypes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtrar esos 5,4 M de genotipos por el mkr_ceph_id de los rs de galanter\n",
    "# ojo con la RAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_2_supp1_Stanford/hgdp/HGDP_Map.txt\"\n",
    "hgdp_stanford_markers = pd.read_csv(fn, sep=\"\\t\", names=[\"dbsnp_id\", \"chr\", \"pos\"],\n",
    "                                    index_col=\"dbsnp_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni of Michigan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import isfile\n",
    "\n",
    "dumpfile = \"./dumpfiles/HGDP_michigan_markers.csv\"\n",
    "\n",
    "if isfile(dumpfile):\n",
    "    dtypes = {\"dbsnp_id\": str, \"chr\": str, \"pos_build_36\": int, \"pos_build_35\": int}\n",
    "    hgdp_michigan_markers = pd.read_csv(dumpfile, index_col=\"dbsnp_id\", dtype=dtypes)\n",
    "else:    \n",
    "    markers_per_chr = []\n",
    "    \n",
    "    # The *.map files were generated with \"parse_HGDP_UMichigan_data.sh\"\n",
    "    for fn in glob(\"~/tesina/HGDP_data/dataset_3_supp2_UMichigan/GENO/chr*.map\"):\n",
    "        markers_per_chr.append(pd.read_csv(fn, sep=\"\\s+\").transpose())\n",
    "\n",
    "    hgdp_michigan_markers = pd.concat(markers_per_chr).drop(0, axis=1)\n",
    "    hgdp_michigan_markers.columns = [\"chr\", \"pos_build_36\", \"pos_build_35\"]\n",
    "    hgdp_michigan_markers[\"pos\"] = hgdp_michigan_markers[\"pos_build_36\"]\n",
    "    hgdp_michigan_markers.index.name = \"dbsnp_id\"\n",
    "    hgdp_michigan_markers.to_csv(dumpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Plank Institute datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_4_supp3_MPlank/hgdpceph.affy500k.map.gz\"\n",
    "hgdp_maxplank_markers = pd.read_csv(fn, sep=\"\\t\", names=[\"chr\", \"dbsnp_id\", \"?\", \"pos\"],\n",
    "                                    usecols=[\"chr\", \"dbsnp_id\", \"pos\"], index_col=\"dbsnp_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_11_supp10_Harvard/Harvard_HGDP-CEPH/all_snp.map.gz\"\n",
    "hgdp_harvard_markers = pd.read_csv(fn, sep=\"\\t\", names=[\"chr\", \"Affy SNP ID\", \"?\", \"pos\"],\n",
    "                                   usecols=[\"chr\", \"Affy SNP ID\", \"pos\"])\n",
    "\n",
    "fn = \"~/tesina/HGDP_data/dataset_11_supp10_Harvard/Axiom_GW_HuOrigin.na35.annot.csv.tar.gz\"\n",
    "affy_human_origins = pd.read_csv(fn, comment=\"#\", skiprows=1, index_col=\"Affy SNP ID\",\n",
    "                                 usecols=[\"Affy SNP ID\", \"dbSNP RS ID\"])\n",
    "\n",
    "hgdp_harvard_markers = hgdp_harvard_markers.set_index(\"Affy SNP ID\")\n",
    "hgdp_harvard_markers = hgdp_harvard_markers.join(affy_human_origins)\n",
    "hgdp_harvard_markers = hgdp_harvard_markers.rename(columns={\"dbSNP RS ID\": \"dbsnp_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_15_supp15_UCLA/snp_info.csv\"\n",
    "hgdp_ucla_markers = pd.read_csv(fn, names=[\"_\", \"_\", \"chr\", \"pos\", \"dbsnp_id\"],\n",
    "                                usecols=[\"chr\", \"pos\", \"dbsnp_id\"], skiprows=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection GAL x HGDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgdp_panels = {\n",
    "    \"CEPH\": hgdp_ceph_markers,\n",
    "    \"Stanford\": hgdp_stanford_markers,\n",
    "    \"MaxPlank\": hgdp_maxplank_markers,\n",
    "    \"Harvard\": hgdp_harvard_markers,\n",
    "    \"UCLA\": hgdp_ucla_markers,\n",
    "    \"UMichigan\": hgdp_michigan_markers,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter_HGDP_matches = pd.DataFrame({\"dbsnp_id\": galanter.index})\n",
    "galanter_HGDP_matches = galanter_HGDP_matches.set_index(\"dbsnp_id\")\n",
    "\n",
    "for panel_name, markers in hgdp_panels.items():\n",
    "    print(panel_name, len(markers))\n",
    "    galanter_HGDP_matches[panel_name] = \\\n",
    "        galanter_HGDP_matches.index.map(lambda x: x in markers.index.values)\n",
    "\n",
    "galanter_HGDP_matches[\"hits\"] = galanter_HGDP_matches.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "galanter_hgdp_indices = {}\n",
    "\n",
    "print(\"Galanter Matches in HGDP panels:\")\n",
    "for hgdp_panel_name, markers in hgdp_panels.items():\n",
    "    matches = galanter_HGDP_matches[hgdp_panel_name]\n",
    "    intersection_count = len(matches[matches])\n",
    "    \n",
    "    if intersection_count > 0:\n",
    "        galanter_hgdp_indices[hgdp_panel_name] = {}\n",
    "        galanter_hgdp_indices[hgdp_panel_name][\"galT\"] = galanter_HGDP_matches[matches].index\n",
    "        galanter_hgdp_indices[hgdp_panel_name][\"galP\"] = \\\n",
    "            galanter_HGDP_matches[matches].loc[present.index].dropna(axis=0).index\n",
    "        \n",
    "        print(\"-\")\n",
    "        print(hgdp_panel_name, \"galT ->\",\n",
    "              len(galanter_hgdp_indices[hgdp_panel_name][\"galT\"]))\n",
    "        print(hgdp_panel_name, \"galP ->\",\n",
    "              len(galanter_hgdp_indices[hgdp_panel_name][\"galP\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGDP populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgdp_continents = {\n",
    "    \"Asia\": \"ASN\",\n",
    "    \"Subsaharian Africa\": \"AFR\",\n",
    "    \"Oceania\": \"EAS\",\n",
    "    \"Europe\": \"EUR\",\n",
    "    \"Middle Est\": \"WAS\",\n",
    "    \"America\": \"AMR\",\n",
    "    \"North Africa\": \"AFR\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEPH populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_1_HGDP-CEPH_v3/hgdp-ceph-pop.out\"\n",
    "usecols = [\"population_name\", \"population_id\", \"nickname\"]\n",
    "hgdp_populations_detail = pd.read_csv(fn, sep=\"\\t\", usecols=usecols)\n",
    "hgdp_populations_detail.set_index(\"population_name\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popcodes = dict(zip(hgdp_populations_detail.index.values,\n",
    "                    hgdp_populations_detail[\"nickname\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_1_HGDP-CEPH_v3/hgdp-ceph-unrelated.out\"\n",
    "hgdp_samples = pd.read_csv(fn, sep=\"\\t\", index_col=\"hgdp_id\")\n",
    "hgdp_samples[\"continent\"] = hgdp_samples[\"Region\"].map(hgdp_continents)\n",
    "hgdp_samples[\"population_name\"] = hgdp_samples[\"population\"]\n",
    "hgdp_samples[\"population\"] = hgdp_samples[\"population_name\"].map(popcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regions = dict(zip(hgdp_samples[\"population_name\"],\n",
    "                   hgdp_samples[\"Region\"]))\n",
    "continents = dict(zip(hgdp_samples[\"population_name\"],\n",
    "                      hgdp_samples[\"continent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hgdp_populations_detail[\"continent\"] = \\\n",
    "    hgdp_populations_detail.index.map(lambda x: continents[x])\n",
    "hgdp_populations_detail[\"region\"] = \\\n",
    "    hgdp_populations_detail.index.map(lambda x: regions[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPlank populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrections = {\n",
    "    \"Colombian\": \"Colombians\",\n",
    "    \"Cambodian\": \"Cambodians\",\n",
    "    \"MbutiPygmy\": \"Mbuti_Pygmies\",\n",
    "    \"NewGuinea\": \"Papuan\",\n",
    "}\n",
    "\n",
    "superpops = {\n",
    "    \"Nasioi\": \"EAS\",\n",
    "    \"BiakaPygmy\": \"AFR\",\n",
    "    \"Bantu\": \"AFR\",\n",
    "    \"Basque\": \"EUR\",\n",
    "    \"Bergamo\": \"EUR\",\n",
    "}\n",
    "\n",
    "def fix_popnames(popname):\n",
    "    if popname not in corrections.keys():\n",
    "        return popname\n",
    "    \n",
    "    return corrections[popname]\n",
    "\n",
    "def get_superpop(popname):\n",
    "    if popname in hgdp_populations_detail.index:\n",
    "        return hgdp_populations_detail.loc[popname][\"continent\"]\n",
    "\n",
    "    if popname in superpops.keys():\n",
    "        return superpops[popname]\n",
    "    \n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_4_supp3_MPlank/hgdpceph.affy500k.pedind\"\n",
    "mplank_populations = pd.read_csv(fn, sep=\"\\s+\",\n",
    "                                 names=[\".\", \"sample\", \".\", \".\", \".\", \"population\"],\n",
    "                                 usecols=[\"sample\", \"population\"])\n",
    "mplank_populations.set_index(\"sample\", inplace=True)\n",
    "mplank_populations[\"population\"] = mplank_populations[\"population\"].map(fix_popnames)\n",
    "mplank_populations[\"continent\"] = mplank_populations[\"population\"].map(get_superpop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGDP genotypes and populations read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hgdp_genotypes = defaultdict(OrderedDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxPlank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = \"~/tesina/HGDP_data/dataset_4_supp3_MPlank/hgdpceph.affy500k.AT.traw.parsed\"\n",
    "df = pd.read_csv(fn, sep=\"\\s+\")\n",
    "renamed_columns = [s.split(\"_\")[-1] for s in df.columns]\n",
    "df.columns = renamed_columns\n",
    "df.rename(columns={\"SNP\": \"dbsnp_id\"}, inplace=True)\n",
    "df.set_index(\"dbsnp_id\", inplace=True)\n",
    "\n",
    "for panel_name in panels:\n",
    "    indices = galanter_hgdp_indices[\"MaxPlank\"][panel_name]\n",
    "    hgdp_genotypes[\"MaxPlank\"][panel_name] = df.loc[indices]\n",
    "\n",
    "df = None # Hope this cleans the big dataframe from memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latinos = ['Colombians', 'Karitiana', 'Maya', 'Surui', 'Pima']\n",
    "europeans = ['Basque', 'Bergamo', 'French']\n",
    "africans = ['BiakaPygmy' 'Bantu', 'Mandenka']\n",
    "middle_eastern = ['Balochi', 'Bedouin', 'Brahui', 'Mongola']\n",
    "oceania = ['Papuan']\n",
    "east_asian = ['Dai']\n",
    "\n",
    "LEA_populations = latinos + africans + europeans\n",
    "world_populations = LEA_populations + middle_eastern + oceania\n",
    "\n",
    "datasets_mplank = OrderedDict()\n",
    "dataset_definitions = {\n",
    "    \"MaxPlank LEA\": LEA_populations,\n",
    "    \"MaxPlank World\": world_populations,\n",
    "}\n",
    "\n",
    "for dataset_label, population_list in dataset_definitions.items():\n",
    "    mask = mplank_populations[\"population\"].isin(population_list)\n",
    "    sample_indices = mplank_populations[mask].index\n",
    "    dataset = hgdp_genotypes[\"MaxPlank\"][\"galT\"].loc[:, sample_indices].transpose()\n",
    "    datasets_mplank[dataset_label] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% run plot_PCAs.py\n",
    "\n",
    "hgdp_panel_name = \"MaxPlank\"\n",
    "\n",
    "panel_indices = OrderedDict()\n",
    "panel_indices['GAL Total'] = galanter.index\n",
    "panel_indices['GAL Parcial'] = present.index\n",
    "\n",
    "for dataset_label, dataset in datasets_mplank.items():\n",
    "    pca = plot_PCAs(dataset_label, panel_indices, dataset,\n",
    "                    mplank_populations, plot_markers, plot_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
